# -*- coding: utf-8 -*-
"""Loan_data_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TvDBbe4MqGC6ivBpn2d_U7axApi8KD-l

Steps to perform:
Perform exploratory data analysis and feature engineering and then apply feature engineering. Follow up with a deep learning model to predict whether or not the loan will be default using the historical data.

Tasks:

1.     Feature Transformation

●	Transform categorical values into numerical values (discrete)

2.     Exploratory data analysis of different factors in the dataset.

3.     Additional Feature Engineering

●	You will check the correlation between features and drop those features that have a strong correlation.
●	This will help reduce the number of features and leave you with the most relevant features.

4.     Modeling

●	After applying EDA and feature engineering, you are now ready to build the predictive models.
●	In this part, you will create a deep learning model using Keras with Tensorflow backend.
"""

# import the library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)
print('imported')

data=pd.read_csv('/content/loan_data.csv')
print('data loaded')

data

# @title purpose

from matplotlib import pyplot as plt
import seaborn as sns
dg = data.groupby('purpose').size()
print(dg)
dg.plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# @title purpose vs log.annual.inc

from matplotlib import pyplot as plt
import seaborn as sns
figsize = (12, 1.2 * len(data['purpose'].unique()))
plt.figure(figsize=figsize)
sns.violinplot(data, x='log.annual.inc', y='purpose', inner='box', palette='Dark2')
sns.despine(top=True, right=True, bottom=True, left=True)

data.info()

data.isnull().sum()

data['not.fully.paid'].value_counts()

plt.figure(figsize=(10,5))
sns.heatmap(data.corr(numeric_only=True),annot=True)

# handle imbalance data
no_fully_paid_0=data[data['not.fully.paid']==0]
no_fully_paid_1=data[data['not.fully.paid']==1]

print(no_fully_paid_0.shape)
print(no_fully_paid_1.shape)

# resample oversampling
from sklearn.utils import resample
no_fully_paid_1_upsampled=resample(no_fully_paid_1,replace=True,n_samples=len(no_fully_paid_0),random_state=42)
new_df=pd.concat([no_fully_paid_0,no_fully_paid_1_upsampled])
new_df['not.fully.paid'].value_counts()

# shuffle the data
from sklearn.utils import shuffle
new_df=shuffle(new_df)
new_df

new_df.info()

# apply label encoder
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
new_df['purpose']=le.fit_transform(new_df['purpose'])

"""# Feature Engineering"""

plt.figure(figsize=(10,5))
sns.heatmap(new_df.corr(numeric_only=True),annot=True)

corr_df= pd.DataFrame(new_df.corr().abs()['not.fully.paid'].sort_values(ascending=False)).reset_index()
corr_df.columns=['feature','corr']
corr_df

# we will choose the features which has a correlation of greater than 0.06
# corr_df[corr_df['corr']>0.06]
corr_df = corr_df[corr_df['corr']>0.06]
columns = corr_df['feature'].values
columns

new_df = new_df[columns]
X = new_df.drop('not.fully.paid',axis=1)
y = new_df['not.fully.paid']

X.shape

y.shape

# create splits
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)

# Apply std scaling
from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
X_train=sc.fit_transform(X_train)
X_test=sc.transform(X_test)

X_train

"""# Create ANN using keras & tf

1. Create Architecture
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Dropout,BatchNormalization
# l2 regularizer
from tensorflow.keras.regularizers import l2
print('imported')

X_train.shape[1]

from tensorflow import keras
model=Sequential()
# this will eliminate UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
inputs = keras.Input(shape=(X_train.shape[1],))
model.add(inputs)
# add first hidden layer
model.add(Dense(units=19,activation='relu',kernel_regularizer=l2(0.001)))
model.add(Dropout(0.2))
model.add(BatchNormalization())
# add scond hidden layer
model.add(Dense(units=15,activation='relu',kernel_regularizer=l2(0.001)))
model.add(Dropout(0.2))
model.add(BatchNormalization())
# third hidden layer
model.add(Dense(units=10,activation='relu',kernel_regularizer=l2(0.001)))
model.add(Dropout(0.2))
model.add(BatchNormalization())
# output
model.add(Dense(1,activation='sigmoid'))
print('model created')

model.summary()

# model compile
model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
print('model compiled')

"""### gpu, over the epoch , accuracy must increase , losses must dec, monitor performance over the epochs, 10 epochs, model callbacks"""

# set up early stopping and model checkpoint call backs and train the model.
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import EarlyStopping
early_stop=EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=10,min_delta=0.01)
checkpoint=ModelCheckpoint('best_model.h5',monitor='val_loss',mode='min',verbose=1,save_best_only=True)
history=model.fit(X_train,y_train,
                  epochs=50,batch_size=128,
                  validation_data=(X_test,y_test),
                  callbacks=[early_stop,checkpoint])

# print test score
score=model.evaluate(X_test,y_test)
print('test loss',score[0])
print('test accuracy',score[1])

# training accuracy
score=model.evaluate(X_train,y_train)
print('train loss',score[0])
print('train accuracy',score[1])

# plot accuracy
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'],loc='upper left')

plt.subplot(1,2,2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'],loc='upper left')
plt.tight_layout()
plt.show()

"""# Apply random forest"""

from sklearn.ensemble import RandomForestClassifier
rf=RandomForestClassifier()
rf.fit(X_train,y_train)

# pred & test
y_pred=rf.predict(X_test)
from sklearn.metrics import confusion_matrix,classification_report
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))

